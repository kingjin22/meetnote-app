import AVFoundation
import Speech
import Foundation

class TranscriptionService {
  private let maxConcurrentTasks = 4
  private let chunkDuration: TimeInterval = 30.0 // 30ì´ˆ ì²­í¬
  private var activeRecognizers: [SFSpeechRecognizer] = []
  private var activeTasks: [SFSpeechRecognitionTask] = []
  
  struct TranscriptionProgress {
    let totalChunks: Int
    let completedChunks: Int
    let currentChunk: Int
    
    var percentage: Double {
      totalChunks > 0 ? Double(completedChunks) / Double(totalChunks) : 0.0
    }
  }
  
  struct TranscriptionResult {
    let text: String
    let error: Error?
  }
  
  func transcribeFile(
    url: URL,
    locale: Locale,
    allowOnlineFallback: Bool,
    onProgress: @escaping (TranscriptionProgress) -> Void,
    completion: @escaping (Result<String, Error>) -> Void
  ) {
    // ê¶Œí•œ í™•ì¸
    guard SFSpeechRecognizer.authorizationStatus() == .authorized else {
      completion(.failure(NSError(
        domain: "TranscriptionService",
        code: -1,
        userInfo: [NSLocalizedDescriptionKey: "ìŒì„± ì¸ì‹ ê¶Œí•œì´ í•„ìš”í•´ìš”."]
      )))
      return
    }
    
    // Recognizer ìƒì„±
    guard let recognizer = SFSpeechRecognizer(locale: locale) else {
      completion(.failure(NSError(
        domain: "TranscriptionService",
        code: -2,
        userInfo: [NSLocalizedDescriptionKey: "ìŒì„± ì¸ì‹ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ìš”."]
      )))
      return
    }
    
    guard recognizer.isAvailable else {
      completion(.failure(NSError(
        domain: "TranscriptionService",
        code: -3,
        userInfo: [NSLocalizedDescriptionKey: "ìŒì„± ì¸ì‹ì´ ì ì‹œ í›„ì— ê°€ëŠ¥í•´ìš”."]
      )))
      return
    }
    
    print("ğŸš€ Starting chunked transcription for: \(url.lastPathComponent)")
    
    // 1. ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì í•©í•œ í¬ë§·ìœ¼ë¡œ ë³€í™˜ (í•„ìš”ì‹œ)
    prepareAudioForRecognition(from: url) { [weak self] preparedURL, tempURL in
      guard let self = self else { return }
      
      // 2. ì˜¤ë””ì˜¤ ê¸¸ì´ í™•ì¸ ë° ì²­í¬ ìƒì„±
      self.splitAudioIntoChunks(url: preparedURL) { result in
        switch result {
        case .success(let chunks):
          print("ğŸ“¦ Created \(chunks.count) chunks")
          
          // 3. ì²­í¬ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬
          self.transcribeChunksInParallel(
            chunks: chunks,
            recognizer: recognizer,
            allowOnlineFallback: allowOnlineFallback,
            onProgress: onProgress
          ) { transcriptionResult in
            // 4. ì„ì‹œ íŒŒì¼ ì •ë¦¬
            for chunk in chunks {
              try? FileManager.default.removeItem(at: chunk.url)
            }
            if let tempURL = tempURL {
              try? FileManager.default.removeItem(at: tempURL)
            }
            
            // 5. ê²°ê³¼ ë°˜í™˜
            switch transcriptionResult {
            case .success(let text):
              print("âœ… Transcription completed: \(text.count) chars")
              completion(.success(text))
            case .failure(let error):
              print("âŒ Transcription failed: \(error.localizedDescription)")
              completion(.failure(error))
            }
          }
          
        case .failure(let error):
          if let tempURL = tempURL {
            try? FileManager.default.removeItem(at: tempURL)
          }
          completion(.failure(error))
        }
      }
    }
  }
  
  private struct AudioChunk {
    let url: URL
    let index: Int
    let startTime: TimeInterval
    let duration: TimeInterval
  }
  
  private func splitAudioIntoChunks(
    url: URL,
    completion: @escaping (Result<[AudioChunk], Error>) -> Void
  ) {
    DispatchQueue.global(qos: .userInitiated).async {
      let asset = AVURLAsset(url: url)
      let duration = asset.duration.seconds
      
      guard duration > 0 else {
        completion(.failure(NSError(
          domain: "TranscriptionService",
          code: -4,
          userInfo: [NSLocalizedDescriptionKey: "ì˜¤ë””ì˜¤ ê¸¸ì´ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ì–´ìš”."]
        )))
        return
      }
      
      print("ğŸµ Audio duration: \(duration)s")
      
      // ì²­í¬ ìˆ˜ ê³„ì‚°
      let numChunks = Int(ceil(duration / self.chunkDuration))
      var chunks: [AudioChunk] = []
      
      // ê° ì²­í¬ ìƒì„±
      for i in 0..<numChunks {
        let startTime = Double(i) * self.chunkDuration
        let chunkDuration = min(self.chunkDuration, duration - startTime)
        
        let tempURL = URL(fileURLWithPath: NSTemporaryDirectory())
          .appendingPathComponent("chunk_\(i)_\(UUID().uuidString)")
          .appendingPathExtension("m4a")
        
        let chunk = AudioChunk(
          url: tempURL,
          index: i,
          startTime: startTime,
          duration: chunkDuration
        )
        chunks.append(chunk)
      }
      
      // ì²­í¬ íŒŒì¼ ìƒì„±
      let group = DispatchGroup()
      var errors: [Error] = []
      
      for chunk in chunks {
        group.enter()
        self.exportAudioChunk(
          asset: asset,
          startTime: chunk.startTime,
          duration: chunk.duration,
          outputURL: chunk.url
        ) { error in
          if let error = error {
            errors.append(error)
          }
          group.leave()
        }
      }
      
      group.notify(queue: .main) {
        if let firstError = errors.first {
          // ì‹¤íŒ¨í•œ ì²­í¬ ì •ë¦¬
          for chunk in chunks {
            try? FileManager.default.removeItem(at: chunk.url)
          }
          completion(.failure(firstError))
        } else {
          completion(.success(chunks))
        }
      }
    }
  }
  
  private func exportAudioChunk(
    asset: AVAsset,
    startTime: TimeInterval,
    duration: TimeInterval,
    outputURL: URL,
    completion: @escaping (Error?) -> Void
  ) {
    guard let exportSession = AVAssetExportSession(
      asset: asset,
      presetName: AVAssetExportPresetAppleM4A
    ) else {
      completion(NSError(
        domain: "TranscriptionService",
        code: -5,
        userInfo: [NSLocalizedDescriptionKey: "Export session ìƒì„± ì‹¤íŒ¨"]
      ))
      return
    }
    
    exportSession.outputURL = outputURL
    exportSession.outputFileType = .m4a
    exportSession.shouldOptimizeForNetworkUse = false
    
    let start = CMTime(seconds: startTime, preferredTimescale: 600)
    let duration = CMTime(seconds: duration, preferredTimescale: 600)
    let timeRange = CMTimeRange(start: start, duration: duration)
    exportSession.timeRange = timeRange
    
    exportSession.exportAsynchronously {
      if exportSession.status == .completed {
        completion(nil)
      } else {
        completion(exportSession.error ?? NSError(
          domain: "TranscriptionService",
          code: -6,
          userInfo: [NSLocalizedDescriptionKey: "ì²­í¬ export ì‹¤íŒ¨"]
        ))
      }
    }
  }
  
  private func transcribeChunksInParallel(
    chunks: [AudioChunk],
    recognizer: SFSpeechRecognizer,
    allowOnlineFallback: Bool,
    onProgress: @escaping (TranscriptionProgress) -> Void,
    completion: @escaping (Result<String, Error>) -> Void
  ) {
    let totalChunks = chunks.count
    var completedChunks = 0
    var results: [Int: String] = [:] // index -> text
    var hasError = false
    var firstError: Error?
    
    let queue = DispatchQueue(label: "com.meetnote.transcription", attributes: .concurrent)
    let semaphore = DispatchSemaphore(value: maxConcurrentTasks)
    let group = DispatchGroup()
    
    // ì´ˆê¸° ì§„í–‰ë¥ 
    onProgress(TranscriptionProgress(
      totalChunks: totalChunks,
      completedChunks: 0,
      currentChunk: 0
    ))
    
    for chunk in chunks {
      guard !hasError else { break }
      
      group.enter()
      queue.async { [weak self] in
        guard let self = self else {
          group.leave()
          return
        }
        
        semaphore.wait() // ë™ì‹œ ì‹¤í–‰ ì œí•œ
        
        print("ğŸ”„ Processing chunk \(chunk.index + 1)/\(totalChunks)")
        
        self.transcribeChunk(
          url: chunk.url,
          recognizer: recognizer,
          allowOnlineFallback: allowOnlineFallback
        ) { result in
          defer {
            semaphore.signal()
            group.leave()
          }
          
          switch result {
          case .success(let text):
            queue.sync(flags: .barrier) {
              guard !hasError else { return }
              
              results[chunk.index] = text
              completedChunks += 1
              
              print("âœ“ Chunk \(chunk.index + 1) done: \(text.prefix(50))...")
              
              // ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
              DispatchQueue.main.async {
                onProgress(TranscriptionProgress(
                  totalChunks: totalChunks,
                  completedChunks: completedChunks,
                  currentChunk: chunk.index + 1
                ))
              }
            }
            
          case .failure(let error):
            queue.sync(flags: .barrier) {
              if !hasError {
                hasError = true
                firstError = error
              }
            }
          }
        }
      }
    }
    
    group.notify(queue: .main) {
      if let error = firstError {
        completion(.failure(error))
      } else {
        // ê²°ê³¼ë¥¼ ìˆœì„œëŒ€ë¡œ ë³‘í•©
        let sortedTexts = (0..<totalChunks).compactMap { results[$0] }
        let finalText = sortedTexts.joined(separator: " ").trimmingCharacters(in: .whitespacesAndNewlines)
        
        if finalText.isEmpty {
          completion(.failure(NSError(
            domain: "TranscriptionService",
            code: -7,
            userInfo: [NSLocalizedDescriptionKey: "í…ìŠ¤íŠ¸ ë³€í™˜ ê²°ê³¼ê°€ ë¹„ì–´ ìˆì–´ìš”."]
          )))
        } else {
          completion(.success(finalText))
        }
      }
    }
  }
  
  private func transcribeChunk(
    url: URL,
    recognizer: SFSpeechRecognizer,
    allowOnlineFallback: Bool,
    completion: @escaping (Result<String, Error>) -> Void
  ) {
    let request = SFSpeechURLRecognitionRequest(url: url)
    request.shouldReportPartialResults = false
    request.requiresOnDeviceRecognition = recognizer.supportsOnDeviceRecognition
    
    var task: SFSpeechRecognitionTask?
    task = recognizer.recognitionTask(with: request) { transcription, error in
      if let error = error {
        // On-device ì‹¤íŒ¨ì‹œ online ì¬ì‹œë„
        if request.requiresOnDeviceRecognition && allowOnlineFallback {
          let retryRequest = SFSpeechURLRecognitionRequest(url: url)
          retryRequest.shouldReportPartialResults = false
          retryRequest.requiresOnDeviceRecognition = false
          
          _ = recognizer.recognitionTask(with: retryRequest) { retryTranscription, retryError in
            if let retryError = retryError {
              completion(.failure(retryError))
            } else if let retryTranscription = retryTranscription, retryTranscription.isFinal {
              let text = retryTranscription.bestTranscription.formattedString
                .trimmingCharacters(in: .whitespacesAndNewlines)
              completion(.success(text))
            }
          }
        } else {
          completion(.failure(error))
        }
        return
      }
      
      guard let transcription = transcription, transcription.isFinal else {
        return
      }
      
      let text = transcription.bestTranscription.formattedString
        .trimmingCharacters(in: .whitespacesAndNewlines)
      completion(.success(text))
    }
  }
  
  private func prepareAudioForRecognition(
    from url: URL,
    completion: @escaping (URL, URL?) -> Void
  ) {
    let extensionLowercased = url.pathExtension.lowercased()
    let speechFriendlyExtensions: Set<String> = ["m4a", "caf", "wav", "aif", "aiff"]
    
    guard !speechFriendlyExtensions.contains(extensionLowercased) else {
      completion(url, nil)
      return
    }
    
    let asset = AVURLAsset(url: url)
    let preset = AVAssetExportPresetAppleM4A
    let compatiblePresets = AVAssetExportSession.exportPresets(compatibleWith: asset)
    
    guard compatiblePresets.contains(preset),
          let exportSession = AVAssetExportSession(asset: asset, presetName: preset) else {
      completion(url, nil)
      return
    }
    
    let tempURL = URL(fileURLWithPath: NSTemporaryDirectory())
      .appendingPathComponent(UUID().uuidString)
      .appendingPathExtension("m4a")
    
    exportSession.outputURL = tempURL
    exportSession.outputFileType = .m4a
    exportSession.shouldOptimizeForNetworkUse = false
    
    exportSession.exportAsynchronously {
      guard exportSession.status == .completed else {
        try? FileManager.default.removeItem(at: tempURL)
        completion(url, nil)
        return
      }
      completion(tempURL, tempURL)
    }
  }
  
  func cancelAll() {
    activeTasks.forEach { $0.cancel() }
    activeTasks.removeAll()
    activeRecognizers.removeAll()
  }
}
